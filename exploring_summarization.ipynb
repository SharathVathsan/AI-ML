{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyEOKoXHUaNQqaL+jL++rj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharathVathsan/AI-ML/blob/NLP/exploring_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0YyEETAMNho",
        "outputId": "5a0339f7-6aaa-4818-b685-52b323bf53a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "KWABlqdpN_il",
        "outputId": "57e6ca06-222f-45fe-cdd6-6acff864195f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.20.2 (from transformers[sentencepiece])\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Installing collected packages: sentencepiece, protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.2 sentencepiece-0.1.99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2B6LQM3L5Mz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(task = 'summarization',model = 'google/pegasus-cnn_dailymail')\n",
        "summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3lzry38MTao",
        "outputId": "ef688bf4-f0dd-4b9a-f711-8eeb4c577eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x7f8fe3e27a30>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer('''\n",
        "            The most important reason I am sharing what are perhaps the most famous\n",
        "            thought experiments in history is as an introduction to using the same approach\n",
        "            with respect to the brain. As you will see, we can get remarkably far in figuring\n",
        "            out how human intelligence works through some simple mind experiments of\n",
        "            our own. Considering the subject matter involved, mind experiments should be a\n",
        "            very appropriate approach.\n",
        "            If a young man's idle thoughts and the use of no equipment other than pen\n",
        "            and paper were sufficient to revolutionize our understanding of physics, then we\n",
        "            should be able to make reasonable progress with a phenomenon with which we\n",
        "            are much more familiar. After all, we experience our thinking every moment of\n",
        "            our waking lives—and our dreaming lives as well.\n",
        "            After we construct a model of how thinking works through this process of\n",
        "            self-reflection, we'll examine to what extent we can confirm it through the latest\n",
        "            observations of actual brains and the state of the art in re-creating these\n",
        "            processes in machines.\n",
        "            ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r32xA_JMxjq",
        "outputId": "645049ee-5129-4c42-93b9-6d911abd16f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'We can get remarkably far in figuring out how human intelligence works through some simple mind experiments of our own .<n>If a young man’s idle thoughts and the use of no equipment other than pen and paper were sufficient to revolutionize our understanding of physics, then we should be able to make reasonable progress with a phenomenon with which we are much more familiar .<n>After we construct a model of how thinking works through this process of self-reflection, we’ll examine to what extent we can confirm it through the latest observations of actual brains and the state of the art in re-creating these processes in machines .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification,TokenClassificationPipeline\n",
        "from transformers.pipelines import AggregationStrategy\n",
        "import numpy as np\n",
        "\n",
        "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
        "    def __init__(self, model, *args, **kwargs):\n",
        "        super().__init__(\n",
        "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "    def postprocess(self, all_outputs):\n",
        "        results = super().postprocess(\n",
        "            all_outputs=all_outputs,\n",
        "            aggregation_strategy=AggregationStrategy.FIRST,\n",
        "        )\n",
        "        return np.unique([result.get(\"word\").strip() for result in results])\n",
        "\n"
      ],
      "metadata": {
        "id": "DqeOgOjOOuVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pipeline\n",
        "model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
        "extractor = KeyphraseExtractionPipeline(model=model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2cNodp7E4qx",
        "outputId": "add32c45-5e03-4e2a-f300-da1416079fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"ml6team/keyphrase-extraction-distilbert-inspec\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-KEY\",\n",
            "    \"1\": \"I-KEY\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"B-KEY\": 0,\n",
            "    \"I-KEY\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
            "\n",
            "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at ml6team/keyphrase-extraction-distilbert-inspec.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--ml6team--keyphrase-extraction-distilbert-inspec/snapshots/9bac7f74b5a14a7660a5c48240a0bc7acfb056a4/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "                • Identifying and designing business processes for automation. \n",
        "                • Design, develop and deploy Robotic Process Automation (RPA) solutions using a variety of software tools, design principles and conventions, centred around Automation Anywhere as the primary RPA software. \n",
        "                • Integrate Automation Anywhere with additional software solutions such as document ingestion, decision engines, workflow, rules engines, etc, to create robust intelligent automation solutions \n",
        "                • Configure RPA processes and objects using core workflow principles in an efficient way; ensure they are easily maintainable and easy to understand \n",
        "                • Document the proposed solution which includes a definition of the user interfaces, functional processes, and data within the proposed system.  \n",
        "                • Analyse and understand existing processes and facilitate change requirements as part of a structured change control process \n",
        "                • Solve day to day issues arising while running automated processes and provide timely resolutions \n",
        "                • Maintain proper documentation for the RPA solutions, test procedures and scenarios during UAT and Production phase  \n",
        "                • Coordinate with process owners, process analysts, business stakeholders and other project team members to understand the as-is process and design the to-be automation process flow. Technical Competencies: \n",
        "                • Experience in programming in VB is a must. Knowledge of programming in Java, Python and/or HTML, DBMS skills and SQL skills will be a plus. \n",
        "                • Experience in GUI automation, SAP Automation (or other large enterprise apps), screen scraping and MS Excel automation using native solutions from Automation Anywhere, as well as other techniques \n",
        "                • Good exposure to error handling and recovery strategies in Automation Anywhere. \n",
        "                • Good application modelling skills and knowledge of different recording mechanisms of Automation Anywhere like object cloning, Optical character recognition, image recognition modes. \n",
        "                • Good knowledge on HTML and CSS concepts to understand the properties of web components for efficient and quick troubleshooting and analysis. Knowledge of local and global variables and interlinked tasks.\n",
        "        ''' \n",
        "\n",
        "keyphrases=extractor(text)\n",
        "\n",
        "print(keyphrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik5Fm95e91pZ",
        "outputId": "5e47a425-add6-46e4-d6de-70cd6c812e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['analysis' 'business stakeholders' 'coordinate' 'core workflow'\n",
            " 'dbms skills' 'decision engines' 'document ingestion' 'error handling'\n",
            " 'gui automation' 'image recognition modes' 'java' 'object cloning'\n",
            " 'optical character recognition' 'process analysts' 'process owners'\n",
            " 'processes' 'production phase' 'programming' 'recovery strategies'\n",
            " 'robotic process automation' 'robust intelligent automation solutions'\n",
            " 'rules engines' 'sap automation' 'screen scraping' 'software tools'\n",
            " 'sql skills' 'structured change control process' 'uat' 'user interfaces'\n",
            " 'web' 'workflow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6zRrB_f8dZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}